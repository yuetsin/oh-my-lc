{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 211\n",
    "\n",
    "我只能说，对于流氓 cases 就应该做流氓优化。（前提是不能作弊，就是说你的方法必须正确，不能是返回死数据那样的作弊）\n",
    "\n",
    "Runtime: 196 ms, faster than 91.48% of Python3 online submissions for Add and Search Word - Data structure design.\n",
    "\n",
    "Memory Usage: 20.9 MB, less than 100.00% of Python3 online submissions for Add and Search Word - Data structure design.\n",
    "\n",
    "跟之前的 Word Search 很像，也是用一个 set 一个 list，一个遍历一个 hash。\n",
    "\n",
    "注意添加的时候检查一下 set 是不是存过了（虽然没有定义删除，但是避免重复省点空间也是好的）\n",
    "\n",
    "流氓 case 来了：先是增加了 N 个长度为 2、3 的 word，然后查询 '.' 即一个长度的单词查了几百遍…\n",
    "\n",
    "这样由于我的词库已经很大了，遍历一遍的时间代价实在太大。\n",
    "\n",
    "所以我只好上流氓优化了：\n",
    "\n",
    "        if word == '.':\n",
    "            for ch in 'abcdefghijklmnopqrstuvwxyz':\n",
    "                if ch in self.setType:\n",
    "                    return True\n",
    "            return False\n",
    "            \n",
    "由于字母表是固定大小的，一下子我们就变成了 O(1) 时间复杂度了…\n",
    "\n",
    "甚至可以稍微做点手脚：如果这次的查询字符串和上一次的一样，就直接把上一次的结果拿过来返回，这也能对付这种流氓 cases…\n",
    "\n",
    "简单说，这个 case 并没有很好的测量出我们写的结构的 PERFORMANCE. 虽然看着很吓人就是了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺便…连 sample_trie.py 都得单独处理 search '.' 的情况…这个 case 太丢人了…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
